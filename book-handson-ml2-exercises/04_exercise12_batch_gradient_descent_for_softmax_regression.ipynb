{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a Batch Gradient descent with early stopping for Softmax Regression \n",
    "# (without using Scikit/learn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "np.random.seed(2042)\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X = np.ones((150,3))\n",
    "X[:, (0,1)] = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = np.eye(3)[iris[\"target\"]]\n",
    "\n",
    "p = np.random.permutation(len(X))\n",
    "train_x, train_y, test_x, test_y = X[p][:120], y[p][:120], X[p][120:], y[p][120:]\n",
    "m = 120 # number of examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train cost is 0.07821214210491331 and test cost is 0.10099538905575528\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "# 3 classes and 2 features+bias \n",
    "weights=np.random.rand(3,3) \n",
    "\n",
    "def _get(X, y):\n",
    "    scores = np.dot(X, np.transpose(weights))\n",
    "    softmax = np.exp(scores) / np.sum(np.exp(scores), axis=1).reshape(-1,1)  * np.ones((1,3))\n",
    "    \n",
    "    _m=X.shape[0]\n",
    "    cost = -np.sum(np.log(softmax) * y)/_m\n",
    "    return scores, softmax, cost\n",
    "\n",
    "test_cost = 100\n",
    "for _ in range(10000000):\n",
    "    scores, softmax, train_cost = _get(train_x, train_y)\n",
    "    _, _, new_test_cost = _get(test_x, test_y)\n",
    "    if new_test_cost > test_cost:\n",
    "        break\n",
    "    test_cost = new_test_cost        \n",
    "    \n",
    "    gradient=np.zeros((3,3))\n",
    "    for _class in range(3):\n",
    "        gradient[_class] = np.sum(\n",
    "            (softmax[:, _class] - train_y[:, _class]).reshape(m,1) * train_x, axis=0\n",
    "            ) / m \n",
    "    weights -= gradient * learning_rate\n",
    "\n",
    "print('Train cost is ' + str(train_cost) + ' and test cost is ' + str(test_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, softmax, _ = _get(test_x, test_y)\n",
    "y_predict = np.argmax(softmax, axis=1)\n",
    "accuracy_score = np.mean(y_predict == np.argmax(test_y, axis=1))\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
